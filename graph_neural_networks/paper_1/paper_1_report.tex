%! Author = ashutosh
%! Date = 5/13/22

\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry}
\usepackage{amsmath, amssymb, enumerate, physics}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{epstopdf}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{amsfonts}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    }

\title{\textbf{Graph Neural Networks for Natural Language Processing}}
\author{Ashutosh Tiwari (ashutiwa@iu.edu)}

\begin{document}
\maketitle

This medium page was generated by \href{https://thunderock.github.io/blogs/introduction.html}{HTML document} which was generated from a \href{https://thunderock.github.io/blog_pdfs/introduction.pdf}{LaTeX document} using \href{https://github.com/michal-h20/make4ht}{make4ht}. So to see this blog in its full glory, view the pdf document.


Graphs in general have been used in natural language processing for a long time. But in most of these cases (example ~\cite{erkan-2006-language}) whether their purpose is graph matching or clustering they suffer from two main problems. First is their low expressive power because in most of cases, these do not capture the structural information and are oblivious to the node and edge features in the graph in consideration. Second issue is that all these different kind of graphs are different and thus their representations are not comparable. Therefore it is very difficult to transfer the knowledge from one graph to another.


Very recently inspired by success of neural networks and graphs, Graph Neural Networks started to gain traction in research circles. In this article we are only focussed for their use in Natural Language Processing, but it is worth noting that they are being increasingly used in other areas as well. 

GNNs learn embeddings for each node (in some case edges) and aggregate those embeddings to produce global graph embeddings. This can be summarized as, $$\boldsymbol{h}_i^{(l)}=f_{filter}(A, \boldsymbol{H}^{(l - 1)})$$ where $A\in \mathbb R^{n\times n}$ is adjacency matrix, $\boldsymbol{H}^{(l-1)}$ and $\boldsymbol{H}^{(l)}$ are the input and output node embeddings at the $l-1$-th GNN layer. $f_{filter}$ can take many forms. Few variations of $f_{filter}$ are \begin{itemize}
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \item \textbf{Spectral based Graph Filters} Several examples of this include Graph Convolutional Networks. Defined as multiplication of a signal $x_i\in\mathbb{R}^n$ with the filter $f_{filter}=diag(\theta)$ parameterized by $\theta\in\mathbb{R}^n$ in the Fourier domain: $$f_{filter}*x_i=Uf(A)U^Tx_i$$ where $U$ is the matrix of eigenvectors of the normalized graph Laplacian $L=I_n-D^{-1/2}AD^{-1/2}$. However in most cases computation of the full eigen-decomposition is very expensive. This is where approximations such as using Chebyshev polynomials and Fourier basis functions come handy and try to approximate this equation.
                                                                                                                                                                                                                                                                                                                                                                                                          \item \textbf{Spatial based Graph Filters} In this case
\end{itemize}

\bibliographystyle{alpha}

\bibliography{ashutiwa}

\end{document}